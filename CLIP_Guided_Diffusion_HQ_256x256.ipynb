{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP Guided Diffusion HQ 256x256.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadnow/Simplified-CLIP-Guided-Diffusion-Colab/blob/main/CLIP_Guided_Diffusion_HQ_256x256.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwMUyt9LHG1"
      },
      "source": [
        "# Simplified CLIP Guided Diffusion with (Planned) Real-ESRGAN upscaling\n",
        "**Alpha 0.5 Release**\n",
        "*Real-ESRGAN support is still in planned development.*\n",
        "---\n",
        "\n",
        "This fork is by sadnow. If you are loading the Notebook directly from this url (https://colab.research.google.com/github/sadnow/Simplified-CLIP-Guided-Diffusion-Colab/blob/main/CLIP_Guided_Diffusion_HQ_256x256.ipynb), you are already receiving the latest updates (kek). In case I mess up a link, the GitHub can also be accessed at Updates can be found on: (https://github.com/sadnow/Simplified-CLIP-Guided-Diffusion-Colab). I welcome any feedback.\n",
        "\n",
        "---\n",
        "\n",
        "Modifications so far: \n",
        "\n",
        "-Colab forms implementation\n",
        "\n",
        "-Load dependencies from drive\n",
        "\n",
        "-Added 512 model toggle (you need a lot of vram)\n",
        "\n",
        "Planned features: Real-ESRGAN upscaling, 3d photo inpainting, more settings\n",
        "\n",
        "---\n",
        "This notebook was brought to you by AnimationKit AI. It upscales videos using Real-ESRGAN and offers RIFE motion smoothing / interpolation. Very early in development, has some big updates coming.\n",
        "\n",
        "---\n",
        "## Credits:\n",
        "\n",
        "\"[Original Diffusion notebook] [b]y Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses OpenAI's 256x256 unconditional ImageNet diffusion model (https://github.com/openai/guided-diffusion) together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images.\"\n",
        "\n",
        "This notebook was forked from the following Notebook: https://twitter.com/rivershavewings/status/1419674445044539400?lang=en"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XIqUfrmvLIhg"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ3rNuAWAewx"
      },
      "source": [
        "#@markdown #Check GPU\n",
        "#@markdown Factory reset if you get below a P100 and want to do a 512 image_size\n",
        "\n",
        "# Check the GPU status\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOj78Yvx8jP"
      },
      "source": [
        "def dir_make(a):\n",
        "  import os.path\n",
        "  from os import path\n",
        "  if not path.exists(a):\n",
        "    print(\"Creating\"+a+\"...\")\n",
        "    !mkdir -p $a\n",
        "\n",
        "def dir_check(a):\n",
        "  import os.path\n",
        "  from os import path\n",
        "  if not path.exists(a):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def file_check(a):\n",
        "  import os.path\n",
        "  from os import path\n",
        "  if not path.isfile(a):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "class anKit:\n",
        "  #def __init__(self):\n",
        "  #  pass\n",
        "  import os.path\n",
        "  from os import path\n",
        "  #video tools\n",
        "  def video_sortFrames(sourceframes, destframes):  #takes frames from input folder, moves to init_frame_storage\n",
        "    dir_make(sourceframes)\n",
        "    dir_check(destframes)\n",
        "    %cd $sourceframes\n",
        "    print(\"Copying frames to \"+destframes+\" for processing...\")\n",
        "    !find -maxdepth 1 -name '*.png' -print0 | xargs -0 cp -t $destframes\n",
        "    %cd $destframes\n",
        "    !find . -type f -name \"*.png\" -execdir bash -c 'mv \"$0\" \"${0##*_}\"' {} \\;  #removes anything not numbers\n",
        "    #print(\"Padding filenames in \"+destframes+\".\")\n",
        "    !rename 's/\\d+/sprintf(\"%05d\",$&)/e' *  #adds padding to numbers\n",
        "    print(\"Finished copying frames to \"+destframes+\".\")\n",
        "  def video_splitFrames(sourcefile, destframes):\n",
        "    check_file(sourcefile)\n",
        "    dir_check(destframes)\n",
        "    !ffmpeg -y -r 1 -i $sourcefile -r 1 $destframes/frame%05d.png\n",
        "  def frames2video(sourceframes,fps,outputmp4):\n",
        "    dir_make(sourceframes)\n",
        "    %cd $sourceframes\n",
        "    #!ffmpeg -r $fps -i '%d_out.png' $outputmp4\n",
        "    !ffmpeg -y -f image2 -pattern_type glob -i '*.png' $outputmp4\n",
        "  def video_runUpscale(esrgan,mpath,scale,input,output):\n",
        "    dir_make(input)\n",
        "    dir_check(output)\n",
        "    %cd $esrgan\n",
        "    #print(mpath)\n",
        "    !python inference_realesrgan.py --model_path $mpath --netscale $scale --input $input --output $output\n",
        "    %cd output\n",
        "    !find . -type f -name \"*_*.png\" -execdir bash -c 'mv \"$0\" \"${0##*_}\"' {} \\;  #removes anything not numbers from filename - needed for rife-frame\n",
        "  def video_compress(input,effects,quality,output):  #needs portable\n",
        "    !ffmpeg -y -i $input $visual_effects -c:v hevc_nvenc -rc vbr -cq $constant_quality -qmin $constant_quality -qmax $constant_quality -b:v 0 $output_path_mp4\n",
        "  def RIFE_video(fps,exp,input,scale,output):\n",
        "    %cd /content/Practical-RIFE\n",
        "    !python3 /content/Practical-RIFE/inference_video.py --fps=$fps --exp=$exp --video=$input --scale $scale --output=$output\n",
        "  def RIFE_frames(fps,exp,input,scale,output):  #not currently working right\n",
        "    %cd /content/Practical-RIFE\n",
        "    input = input + '/'\n",
        "    !python3 /content/Practical-RIFE/inference_video.py --fps=$fps --exp=$exp --img=$input --scale $scale --output=$output\n",
        "  def detect_fps(input): #needs portable\n",
        "    import re\n",
        "    fps_ffprobe = !ffprobe -v error -select_streams v -of default=noprint_wrappers=1:nokey=1 -show_entries stream=avg_frame_rate $input\n",
        "    fps_unfinished = [str(i) for i in fps_ffprobe] # Converting integers into strings\n",
        "    fps_unfinishedTwo = str(\"\".join(fps_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', fps_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    fps = int(a_string)\n",
        "    #print(\"Detected FPS is\",fps)\n",
        "    return fps\n",
        "  def detect_duration(input):  #needs portable\n",
        "    import re\n",
        "    duration_ffprobe = !ffprobe -v error -select_streams v:0 -show_entries stream=duration -of default=noprint_wrappers=1:nokey=1 $input\n",
        "    duration_unfinished = [str(i) for i in duration_ffprobe] # Converting integers into strings\n",
        "    duration_unfinishedTwo = str(\"\".join(duration_unfinished)) # Join the string values into one string\n",
        "    numbers = re.findall('[0-9]+', duration_unfinishedTwo)\n",
        "    newNum = numbers[0:1]\n",
        "    strings = [str(integer) for integer in newNum]\n",
        "    a_string = \"\".join(strings)\n",
        "    duration = int(a_string)\n",
        "    #print(\"Detected duration INTEGER (in seconds) is\",duration)\n",
        "    return duration\n",
        "  def exp_calc(): #needs portable\n",
        "    import numpy as np\n",
        "    a = measured_fps * measured_duration\n",
        "    b = target_fps * target_length_seconds\n",
        "    c = b / a\n",
        "    l = np.log(c) / np.log(2)\n",
        "    print(\"Un-rounded --exp is\",l)\n",
        "    x = round(l)\n",
        "    if x < 1:\n",
        "      x = 1\n",
        "    print(\"Rounding up to an --exp of \",x)\n",
        "    return x\n",
        "  def image_upscale(model_path,scale,input,output):\n",
        "    %cd /content/Real-ESRGAN/\n",
        "    #print(mpath)\n",
        "    !python inference_realesrgan.py --model_path $model_path --netscale $scale --input $input --output $output\n",
        "    %cd /content/\n",
        "  #\n",
        "  #\n",
        "  #\n",
        "  #installation for Colab (untested on other platforms)\n",
        "  #\n",
        "  #\n",
        "  def install_ESRGAN():\n",
        "    %cd /content/\n",
        "    print(\"Installing libraries for Real-ESRGAN upscaling.\")\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd /content/Real-ESRGAN\n",
        "    !pip install basicsr -q\n",
        "    !pip install facexlib -q\n",
        "    !pip install gfpgan -q\n",
        "    !pip install -r requirements.txt -q\n",
        "    %cd /content/Real-ESRGAN\n",
        "    !python setup.py develop -q\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth -P experiments/pretrained_models\n",
        "    print(\"Finished Installing libraries for Real-ESRGAN upscaling.\")\n",
        "    %cd /content/\n",
        "    #\n",
        "  def install_RIFE():\n",
        "    %cd /content/\n",
        "    print(\"Installing libraries for RIFE motion smoothing.\")\n",
        "    !git clone https://github.com/hzwer/Practical-RIFE Practical-RIFE\n",
        "    !gdown --id 1O5KfS3KzZCY3imeCr2LCsntLhutKuAqj\n",
        "    #%cd /content/Practical-RIFE\n",
        "    !7z e RIFE_trained_model_v3.8.zip\n",
        "    #!7z e /content/Practical-RIFE/RIFE_trained_model_v3.8.zip\n",
        "    #!7z e /content/RIFE_trained_model_v3.8.zip\n",
        "    !mkdir /content/Practical-RIFE/train_log\n",
        "    !mv *.py /content/Practical-RIFE/train_log/\n",
        "    !mv *.pkl /content/Practical-RIFE/train_log/\n",
        "    %cd /content/Practical-RIFE/\n",
        "    !gdown --id 1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc #useless - mp4 demo\n",
        "    !pip3 install -r requirements.txt\n",
        "    print(\"Finsihed Installing libraries for RIFE motion smoothing.\")\n",
        "    #  \n",
        "    %cd /content/\n",
        "  def depcrecated_installOldRIFE():\n",
        "    print(\"Installing libraries for RIFE motion smoothing.\")\n",
        "    !git clone https://github.com/hzwer/arXiv2020-RIFE RIFE\n",
        "    !gdown --id 1wsQIhHZ3Eg4_AfCXItFKqqyDMB4NS0Yd\n",
        "    !7z e RIFE_trained_model_HDv2.zip\n",
        "    !mkdir /content/RIFE/train_log\n",
        "    !mv *.pkl /content/RIFE/train_log/\n",
        "    %cd /content/RIFE/\n",
        "    !gdown --id 1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc\n",
        "    !pip3 install -r requirements.txt\n",
        "    print(\"Done.\")\n",
        "    print(\"Finsihed Installing libraries for RIFE motion smoothing.\")\n",
        "  %cd /content/\n",
        "    \n",
        "###\n",
        "\n",
        "#@markdown Install, import, load more functions (Required)\n",
        "##mount_google_drive = True #@param {type:\"boolean\"}\n",
        "load_from_drive = True #@param {type:\"boolean\"}\n",
        "drive_install_location = \"/content/drive/MyDrive/z_Colab/Simplified-Diffusion_CLIP\"#@param {type:\"string\"}\n",
        "\n",
        "#@markdown `load_from_drive` is optional. It will save the big files to your specified `drive_install_location`. Next time you run this cell, it will load those files from your Drive. Can save you a minute or two. \n",
        "\n",
        "load_512_model = True #@param {type:\"boolean\"}\n",
        "load_esrgan = True #@param {type:\"boolean\"}\n",
        "\n",
        "def dl_dependencies():\n",
        "  print(\"Downloading dependencies...\")\n",
        "  !git clone https://github.com/openai/CLIP\n",
        "  !git clone https://github.com/crowsonkb/guided-diffusion\n",
        "  # Download the diffusion model\n",
        "  !curl -OL 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt'\n",
        "  if load_512_model:\n",
        "    !curl -OL --http1.1 'https://the-eye.eu/public/AI/models/512x512_diffusion_unconditional_ImageNet/512x512_diffusion_uncond_finetune_008100.pt'\n",
        "\n",
        "\n",
        "def install_dependencies():\n",
        "    !pip install -e ./CLIP -q\n",
        "    !pip install -e ./guided-diffusion -q\n",
        "    !pip install lpips -q\n",
        "    if load_esrgan:\n",
        "      anKit.install_ESRGAN()\n",
        "\n",
        "# Install dependencies loading from Drive if checked\n",
        "clip_exists = dir_check('/content/CLIP')\n",
        "if not clip_exists:\n",
        "  print(\"Installing dependencies...\")\n",
        "  if load_from_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_install_location_clip = drive_install_location + \"/CLIP\"\n",
        "    drive_install_location_exists = dir_check(drive_install_location_clip)\n",
        "    if not drive_install_location_exists:  #if driveinstalllocation isnt present\n",
        "      dir_make(drive_install_location)\n",
        "      print(\"Downloading dependencies...\")\n",
        "      dl_dependencies()\n",
        "      print(\"Copying files to drive_install_location...\")\n",
        "      !rsync -av --progress /content/ $drive_install_location --exclude drive\n",
        "    else:\n",
        "      #this means the drive dir exists\n",
        "      print(\"Loading dependencies from drive (Drive to /content/)...\")\n",
        "      !rsync -av --progress $drive_install_location/ /content/ --exclude drive\n",
        "  else:\n",
        "    dl_dependencies()\n",
        "else:\n",
        "  print(\"/content/CLIP is already present. Skipping dependency download...\")\n",
        "install_dependencies()\n",
        "\n",
        "\n",
        "\n",
        "# Imports\n",
        "\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./guided-diffusion')\n",
        "\n",
        "import clip\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "\n",
        "\n",
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnQjGugaDZPJ",
        "cellView": "form"
      },
      "source": [
        "\n",
        "\n",
        "#@markdown Model settings\n",
        "steps_per_prompt='25' #@param ['25','50','150','250','500','1000','ddim25','ddim50','ddim150','ddim250','ddim500','ddim1000']\n",
        "#timestep_respacing = steps_per_prompt #same thing, different name\n",
        "#@markdown Modify this value to change iterations/prompt. `Sacrifices accuracy/alignment for improved quicker runtime.`\n",
        "diffusion_steps = 1000\n",
        "image_size = '512' #@param [64,128,256,512]\n",
        "\n",
        "\n",
        "\n",
        "image_size=int(image_size)\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': steps_per_prompt,\n",
        "    'image_size': image_size,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_fp16': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "})\n",
        "\n",
        "\n",
        "# Load models\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "\n",
        "#512 model addition\n",
        "if image_size == 512:\n",
        "  model.load_state_dict(torch.load('512x512_diffusion_uncond_finetune_008100.pt', map_location='cpu'))\n",
        "else:\n",
        "  model.load_state_dict(torch.load('256x256_diffusion_uncond.pt', map_location='cpu'))\n",
        "\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "        param.requires_grad_()\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()\n",
        "\n",
        "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "clip_size = clip_model.visual.input_resolution\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9hTc8YLoLx"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5gODNAMEUCR"
      },
      "source": [
        "#@markdown #Input Settings\n",
        "prompt_input = \"17th century Japanese dojo | 4k isometric render | beautiful background skies for| rain drops and lush mountains\" #@param {type:\"string\"}\n",
        "#@markdown `prompt_input` currently doesn't support weights-\n",
        "display_rate = 1 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "#@markdown `display_rate` controls how often \n",
        "prompts = \"['\" + prompt_input + \"']\"\n",
        "prompts = [prompt_input]\n",
        "\n",
        "image_prompts = []\n",
        "##@markdown Default is blank\n",
        "\n",
        "n_batches = 1\n",
        "init_image = None   # This can be an URL or Colab local path and must be in quotes.\n",
        "skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
        "                    # Higher values make the output look more like the init.\n",
        "init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
        "seed = 0\n",
        "\n",
        "cutn = 32#@param {type:\"raw\"}\n",
        "#@markdown `cutn` is the number of randomly cut patches to distort from diffusion. Default 16. 512 model default is 32.\n",
        "batch_size = 1\n",
        "if load_512_model:\n",
        "  cutn_batches = 4  #added, experimental\n",
        "  cut_pow = 0.5\n",
        "  #cutn = (cutn * 2)\n",
        "\n",
        "###########################################\n",
        "#@markdown ---\n",
        "#@markdown ##Untested Settings\n",
        "\n",
        "\n",
        "\n",
        "clip_guidance_scale =  1000#@param {type:\"raw\"}\n",
        "#@markdown `clip_guidane scale` controls how much the image should look like the prompt.\n",
        "tv_scale = 100 #@param {type:\"raw\"}\n",
        "#@markdown `tv_scale` Controls the smoothness of the final output.\n",
        "range_scale = 50 #@param {type:\"raw\"}\n",
        "#@markdown `range_scale` controls how far out of range RGB values are allowed to be.\n",
        "if image_size == 64:\n",
        "  #when using the 64x64 checkpoint, the cosine noise scheduler is used. \n",
        "  #For unclear reasons, this noise scheduler requires different values \n",
        "  #for --clip_guidance_scale and --tv_scale. I recommend starting with -cgs 5 -tvs 0.00001 and\n",
        "  #experimenting from around there. --clip_guidance_scale and --tv_scale will require experimentation.\n",
        "  tv_scale = (tv_scale / 10000000)\n",
        "  clip_guidance_scale = (clip_guidance_scale / 200)\n",
        "\n",
        "###################################################\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ##Real-ESRGAN Upscaling\n",
        "run_upscaler = True #@param{type:\"boolean\"}\n",
        "model_path='/content/Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth' #@param ['/content/Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth','/content/Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x4plus.pth','/content/Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x2plus.pth'] {type:\"string\"}\n",
        "#@markdown Highly recommended! `run_upscaler` will run Real-ESRGAN at the end of diffusion.\n",
        "#upscaling\n",
        "if model_path == '/content/Real-ESRGAN/experiments/pretrained_models/RealESRGAN_x2plus.pth':\n",
        "  upscale_value=\"2\"\n",
        "else:\n",
        "  upscale_value=\"4\"\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ##Output Settings\n",
        "image_output_folder = '/content/drive/MyDrive/z_Colab/Simplified-Diffusion_CLIP/images_out' #@param{type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "def do_run_lowres():\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn)\n",
        "    side_x = side_y = model_config['image_size']\n",
        "\n",
        "    target_embeds, weights = [], []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = clip_model.encode_image(normalize(batch)).float()\n",
        "        target_embeds.append(embed)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    def cond_fn(x, t, y=None):\n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            n = x.shape[0]\n",
        "            my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
        "            out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
        "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "            image_embeds = clip_model.encode_image(clip_in).float()\n",
        "            dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "            dists = dists.view([cutn, n, -1])\n",
        "            losses = dists.mul(weights).sum(2).mean(0)\n",
        "            tv_losses = tv_loss(x_in)\n",
        "            range_losses = range_loss(out['pred_xstart'])\n",
        "            loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
        "            if init is not None and init_scale:\n",
        "                init_losses = lpips_model(x_in, init)\n",
        "                loss = loss + init_losses.sum() * init_scale\n",
        "            return -torch.autograd.grad(loss, x)[0]\n",
        "\n",
        "    if model_config['timestep_respacig'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            model,\n",
        "            (batch_size, 3, side_y, side_x),\n",
        "            clip_denoised=False,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=True,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init,\n",
        "            randomize_class=True,\n",
        "        )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            cur_t -= 1\n",
        "            if j % display_rate == 0 or cur_t == -1:\n",
        "                print()\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
        "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                    display.display(display.Image(filename))\n",
        "\n",
        "\n",
        "def do_run_highres():\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn, cut_pow)\n",
        "    side_x = side_y = model_config['image_size']\n",
        "\n",
        "    target_embeds, weights = [], []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = clip_model.encode_image(normalize(batch)).float()\n",
        "        target_embeds.append(embed)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    def cond_fn(x, t, y=None):\n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            n = x.shape[0]\n",
        "            my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
        "            out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
        "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "            x_in_grad = torch.zeros_like(x_in)\n",
        "            for i in range(cutn_batches):\n",
        "                clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "                image_embeds = clip_model.encode_image(clip_in).float()\n",
        "                dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "                dists = dists.view([cutn, n, -1])\n",
        "                losses = dists.mul(weights).sum(2).mean(0)\n",
        "                x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n",
        "            tv_losses = tv_loss(x_in)\n",
        "            range_losses = range_loss(out['pred_xstart'])\n",
        "            loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
        "            if init is not None and init_scale:\n",
        "                init_losses = lpips_model(x_in, init)\n",
        "                loss = loss + init_losses.sum() * init_scale\n",
        "            x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
        "            grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
        "            return grad\n",
        "\n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            model,\n",
        "            (batch_size, 3, side_y, side_x),\n",
        "            clip_denoised=False,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=True,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init,\n",
        "            randomize_class=True,\n",
        "        )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            #if j % 100 == 0 or cur_t == 0:\n",
        "            if j % display_rate == 0 or cur_t == 0:\n",
        "\n",
        "                print()\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
        "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                    display.display(display.Image(filename))\n",
        "            cur_t -= 1\n",
        "\n",
        "##\n",
        "gc.collect()\n",
        "\n",
        "unscaled_image = '/content/progress_00000.png'\n",
        "\n",
        "if load_512_model:\n",
        "  do_run_highres()\n",
        "else:\n",
        "  do_run_lowres\n",
        "torch.cuda.empty_cache()\n",
        "if run_upscaler:\n",
        "  #anKit.install_ESRGAN()\n",
        "  anKit.image_upscale(model_path,upscale_value,unscaled_image,'/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx-lXGPMX4GX",
        "cellView": "form"
      },
      "source": [
        "#@markdown #Preview upscaled result\n",
        "from IPython.display import Image\n",
        "upscaled_image = '/content/progress_00000_out.png'\n",
        "Image(upscaled_image, width=900, height=900)\n",
        "dir_make(image_output_folder)\n",
        "!cp $upscaled_image $image_output_folder\n",
        "print('Your outputted file is saved as ',image_output_folder)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWYX3MNnmndE"
      },
      "source": [
        "anKit.install_ESRGAN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGksq9kuWzwq"
      },
      "source": [
        "This Notebook has been designed so you can \"Run All.\"\n",
        "\n",
        "Control + F9\n",
        "\n",
        "If you run out of VRAM (CUDA Error), do a \"Reset and Run All.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa2moU6HPEFS"
      },
      "source": [
        "%cd $image_output_folder\n",
        "ls -t | head -n1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}